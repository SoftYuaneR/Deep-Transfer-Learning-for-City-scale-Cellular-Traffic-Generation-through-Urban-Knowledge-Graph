import os, sys
import time
import copy
import matplotlib
import random
matplotlib.use('SVG')
import torch
import math
import warnings
import torch.nn as nn
from torch.nn.utils import weight_norm
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.autograd as autograd
import numpy as np
from scipy.spatial import distance
from matplotlib import pyplot as plt
from tqdm import trange
import setproctitle  
setproctitle.setproctitle("TransGAN_train@hsd")
#os.environ["CUDA_VISIBLE_DEVICES"] = '2,3,6'
gpu=6
torch.manual_seed(5)
use_cuda = torch.cuda.is_available()
USE_KGE = True

class MyDataset_old(Dataset):
    def __init__(self, data, node_data, label_num_idx=0):
        self.data = np.load(data)
        self.node_embedding = node_data / np.mean(node_data)#node_embedding.repeat(4, 1).reshape(nebd_size[0]*4, nebd_size[1]) / np.mean(node_embedding)
        self.bs_record = torch.from_numpy(self.data['bs_record'].astype(np.float32)).reshape(
                self.node_embedding.shape[0], 1, 168)
        # self.kge = torch.from_numpy(self.data['bs_kge'].astype(np.float32))/40.0
        self.kge = torch.from_numpy(self.node_embedding.astype(np.float32))
        self.hours_in_weekday = torch.from_numpy(self.data['hours_in_weekday'].astype(np.float32))
        self.hours_in_weekend = torch.from_numpy(self.data['hours_in_weekend'].astype(np.float32))
        self.days_in_weekday = torch.from_numpy(self.data['days_in_weekday'].astype(np.float32))
        self.days_in_weekend = torch.from_numpy(self.data['days_in_weekend'].astype(np.float32))
        self.days_in_weekday_residual = torch.from_numpy(self.data['days_in_weekday_residual'].astype(np.float32))
        self.days_in_weekend_residual = torch.from_numpy(self.data['days_in_weekend_residual'].astype(np.float32))
        #self.weeks_in_month_residual = torch.from_numpy(self.data['weeks_in_month_residual'].astype(np.float32))
        self.hours_in_weekday_patterns = torch.from_numpy(self.data['hours_in_weekday_patterns'].astype(np.float32))
        self.hours_in_weekend_patterns = torch.from_numpy(self.data['hours_in_weekend_patterns'].astype(np.float32))
        self.days_in_weekday_patterns = torch.from_numpy(self.data['days_in_weekday_patterns'].astype(np.float32))
        self.days_in_weekend_patterns = torch.from_numpy(self.data['days_in_weekend_patterns'].astype(np.float32))
        self.days_in_weekday_residual_patterns = torch.from_numpy(
            self.data['days_in_weekday_residual_patterns'].astype(np.float32))
        self.days_in_weekend_residual_patterns = torch.from_numpy(
            self.data['days_in_weekend_residual_patterns'].astype(np.float32))

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        #bs_id = self.bs_id[idx]
        bs_record = self.bs_record[idx]
        kge = self.kge[idx]
        hours_in_weekday = self.hours_in_weekday[idx]
        hours_in_weekend = self.hours_in_weekend[idx]
        days_in_weekday = self.days_in_weekday[idx]
        days_in_weekend = self.days_in_weekend[idx]
        return idx, bs_record, kge, hours_in_weekday, hours_in_weekend, days_in_weekday, days_in_weekend

    def __len__(self):
        return self.node_embedding.shape[0]


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor
		
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])

class TransBlock(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation="relu", num_layers=6, norm=None):
        super(TransBlock, self).__init__()
        self.layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)
        self.layers = _get_clones(self.layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        #self.apply(self._init_weights)
		
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
#         elif isinstance(m, nn.Conv2d):
#             trunc_normal_(m.weight, std=.02)
#             if isinstance(m, nn.Conv2d) and m.bias is not None:
#                 nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        
    def forward(self, x):
        for lyr in self.layers:
            x = lyr(x)
        if self.norm is not None:
            x = self.norm(x)
        return x


class Generator(nn.Module):
    def __init__(self, nhead=[1,1,1], dim_feedforward=2048, dropout=0.1, activation="relu", num_layers=6):
        super(Generator, self).__init__()
        self.linear =  nn.Linear(NOISE_SIZE+KGE_SIZE, SHAPE[0][0]*SHAPE[0][1]) if USE_KGE else\
                       nn.Linear(NOISE_SIZE, SHAPE[0][0]*SHAPE[0][1])
        self.block_month = TransBlock(SHAPE[0][1], nhead[0], dim_feedforward, dropout, activation, num_layers)
        self.block_week = TransBlock(SHAPE[1][1], nhead[1], dim_feedforward, dropout, activation, num_layers)
        self.block_day = TransBlock(SHAPE[2][1], nhead[2], dim_feedforward, dropout, activation, num_layers)
        self.blocks = nn.ModuleList([ self.block_month, self.block_week, self.block_day ])
        self.relu = nn.ReLU()
        #if SHAPE_M0 > 1:
        #    self.linear_out = nn.Linear(SHAPE_M0, 1)


    def forward(self, x):
        BZ = x.shape[0]
        x = self.linear(x) if USE_KGE else self.linear(x[:,0:32])
        for index, blk in enumerate(self.blocks):
            x = blk(x.view((SHAPE[index][0], BZ, SHAPE[index][1])))
        #print(x.max(), x.min())
        if SHAPE_M0 > 1:
            x = x.reshape(-1, BZ, 1)#self.linear_out(x)
        #print(x.shape)
        return self.relu(x)

class Discriminator(nn.Module):
    def __init__(self, nhead=[1,1,1], dim_feedforward=2048, dropout=0.1, activation="relu", num_layers=6):
        super(Discriminator, self).__init__()
        self.linear =  nn.Linear(int(SHAPE[0][0]*SHAPE[0][1]) + KGE_SIZE, 2048) if USE_KGE else\
                       nn.Linear(int(SHAPE[0][0]*SHAPE[0][1]), 2048)
        self.linear1 =  nn.Linear(2048, 512)
        self.linear2 =  nn.Linear(512, 16)
        self.linear3 =  nn.Linear(16, 1)
        self.relu = nn.ReLU()
        self.block_month = TransBlock(int(SHAPE[0][1]/SHAPE_M0), nhead[0], dim_feedforward, dropout, activation, num_layers)
        self.block_week = TransBlock(int(SHAPE[1][1]/SHAPE_M0), nhead[1], dim_feedforward, dropout, activation, num_layers)
        self.block_day = TransBlock(int(SHAPE[2][1]/SHAPE_M0), nhead[2], dim_feedforward, dropout, activation, num_layers)
        self.blocks = nn.ModuleList([ self.block_day, self.block_week, self.block_month ])

    def forward(self, x, kge):        
        BZ = kge.shape[0]
        #xx = torch.tensor()
        for index, blk in enumerate(self.blocks):
            # x = blk(x.view(int((SHAPE[len(SHAPE)-1-index][0]*SHAPE_M0), BZ, int(SHAPE[len(SHAPE)-1-index][1]/SHAPE_M0))))
             x = blk(x.view((SHAPE[len(SHAPE)-1-index][0]*SHAPE_M0, BZ, int(SHAPE[len(SHAPE)-1-index][1]/SHAPE_M0))))
        #    xx = 
        #x = self.linear(torch.cat((x.permute(1,0,2).reshape(BZ, -1), kge), 1))      
          #   x = self.linear(torch.cat((x.permute(1,0).reshape(BZ, -1), kge), 1))
        #print(x.shape)
             x = torch.cat((x.permute(1,0,2).reshape(BZ, -1), kge), 1) if USE_KGE else x.permute(1,0,2).reshape(BZ, -1)
        #print(x.shape)
        #print(int(SHAPE[0][0]*SHAPE[0][1]/nhead[0]), KGE_SIZE)
             x = self.relu(self.linear3(self.linear2(self.linear1(self.linear(x)))))     
        return x



def calc_gradient_penalty(netD, real_data, fake_data, kge):
    #use_cuda = 0
    LAMBDA = 10
    alpha = torch.rand(BATCH_SIZE, 1)
    alpha = alpha.expand(real_data.size())
    alpha = alpha.cuda(gpu) if use_cuda else alpha

    interpolates = alpha * real_data + ((1 - alpha) * fake_data)

    if use_cuda:
        interpolates = interpolates.cuda(gpu)
    interpolates = autograd.Variable(interpolates, requires_grad=True)

    disc_interpolates = netD(interpolates, kge)
    
    # TODO: Make ConvBackward diffentiable
    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,
                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(
                                  disc_interpolates.size()),
                              create_graph=True, retain_graph=True, only_inputs=True)[0]

    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA
    return gradient_penalty

def generate_data(netG, kge, gene_size=128):
    #noise_attribute = kge#torch.rand(gene_size,12)
    noise = torch.randn(gene_size, NOISE_SIZE)#torch.randn(BATCH_SIZE, NOISE_SIZE, dim_list_g[0])
    if use_cuda:
        #noise_attribute = noise_attribute.cuda(gpu)
        noise = noise.cuda(gpu) 
        kge = kge.cuda(gpu)
    #noisev = autograd.Variable(noise, volatile=True)
    #noise_kge = torch.cat((noise, kge.view(kge.size(0),kge.size(1),1).expand(-1, -1, LENGTH)), 1)
    output = netG(torch.cat((noise, kge), 1))#, kge)
    #print(output.max(), output.min())
    #return output, kge, output_d, output_w, output_m, z
    return output
def distribution_jsd(generated_data, real_dataset, save_dir='', n_bins=100):
    generated_data[generated_data<0]=0
    fig, ax = plt.subplots(figsize=(24, 6))
    line_w = 2
    use_cumulative = -1
    use_log = True
    n_real, bins, patches = ax.hist(real_dataset.flatten(), n_bins, density=True, histtype='step',
                                    cumulative=use_cumulative, label='real', log=use_log, facecolor='g',
                                    linewidth=line_w)
    n_gene, bins, patches = ax.hist(generated_data.flatten(), n_bins, density=True, histtype='step',
                                    cumulative=use_cumulative, label='gene', log=use_log, facecolor='b',
                                    linewidth=line_w)
    JSD = distance.jensenshannon(n_real.flatten(), n_gene.flatten(), 2.0)
    ax.grid(True)
    ax.legend(loc='right')
    ax.set_title('Cumulative step histograms')
    ax.set_xlabel('Value')
    ax.set_ylabel('Likelihood of occurrence')
    if save_dir:
        plt.savefig(os.path.join(save_dir, 'fig_hist.jpg'))
    plt.close()

    fig, ax = plt.subplots(figsize=(24, 6))
    real_diff = real_dataset[1:] - real_dataset[:-1]
    generated_diff = generated_data[1:] - generated_data[:-1]
    n_real, bins, patches = ax.hist(real_diff.flatten(), n_bins, density=True, histtype='step',
                                    cumulative=use_cumulative, label='real', log=use_log, facecolor='g',
                                    linewidth=line_w)
    n_gene, bins, patches = ax.hist(generated_diff.flatten(), n_bins, density=True, histtype='step',
                                    cumulative=use_cumulative, label='gene', log=use_log, facecolor='b',
                                    linewidth=line_w)
    ax.grid(True)
    ax.legend(loc='right')
    ax.set_title('Cumulative step histograms')
    ax.set_xlabel('Value')
    ax.set_ylabel('Likelihood of occurrence')
    if save_dir:
        plt.savefig(os.path.join(save_dir, 'fig_diff_hist.jpg'))
    plt.close()
    JSD_diff = distance.jensenshannon(n_real.flatten(), n_gene.flatten(), 2.0)
    return JSD, JSD_diff

def pattern_freq_ratio_rmse(generated_data, real_dataset):
    data_f = abs(np.fft.rfft(real_dataset))
    daily_real = data_f[:, 7] / data_f.sum(1)

    data_f = abs(np.fft.rfft(generated_data))
    daily = data_f[:, 7] / data_f.sum(1) if data_f.sum(1).all() > 0 else data_f[:, 7]
    daily = np.concatenate([daily,daily,daily,daily])

    rmse_daily = np.sqrt(np.mean((daily - daily_real) ** 2))
    return rmse_daily
from RESGAN_Partly import MyDataset

class TransferDataset(Dataset):
    def __init__(self, data, node_data, sample_rate=1):
        self.data = np.load(data)
        self.node_embedding = node_data / np.mean(node_data)#node_embedding.repeat(4, 1).reshape(nebd_size[0]*4, nebd_size[1]) / np.mean(node_embedding)
        sample_list = np.arange(self.node_embedding.shape[0])
        if sample_rate < 1:
            sample_list = np.random.choice(sample_list, int(self.node_embedding.shape[0]*sample_rate), replace=False)
        self.bs_record = torch.from_numpy(self.data['bs_record'].astype(np.float32))#.reshape(
              #  int(self.node_embedding.shape[0]*sample_rate), 1, LENGTH)
        # self.kge = torch.from_numpy(self.data['bs_kge'].astype(np.float32))/40.0
        self.kge = torch.from_numpy(self.node_embedding[sample_list].astype(np.float32))
        self.hours_in_weekday = torch.from_numpy(self.data['hours_in_weekday'].astype(np.float32))
        self.hours_in_weekend = torch.from_numpy(self.data['hours_in_weekend'].astype(np.float32))
        self.days_in_weekday = torch.from_numpy(self.data['days_in_weekday'].astype(np.float32))
        self.days_in_weekend = torch.from_numpy(self.data['days_in_weekend'].astype(np.float32))
        self.days_in_weekday_residual = torch.from_numpy(self.data['days_in_weekday_residual'].astype(np.float32))
        self.days_in_weekend_residual = torch.from_numpy(self.data['days_in_weekend_residual'].astype(np.float32))
        #self.weeks_in_month_residual = torch.from_numpy(self.data['weeks_in_month_residual'][sample_list].astype(np.float32))
        self.hours_in_weekday_patterns = torch.from_numpy(self.data['hours_in_weekday_patterns'].astype(np.float32))
        self.hours_in_weekend_patterns = torch.from_numpy(self.data['hours_in_weekend_patterns'].astype(np.float32))
        self.days_in_weekday_patterns = torch.from_numpy(self.data['days_in_weekday_patterns'].astype(np.float32))
        self.days_in_weekend_patterns = torch.from_numpy(self.data['days_in_weekend_patterns'].astype(np.float32))
        self.days_in_weekday_residual_patterns = torch.from_numpy(
            self.data['days_in_weekday_residual_patterns'].astype(np.float32))
        self.days_in_weekend_residual_patterns = torch.from_numpy(
            self.data['days_in_weekend_residual_patterns'].astype(np.float32))

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        #bs_id = self.bs_id[idx]
        bs_record = self.bs_record[idx]
        kge = self.kge[idx]
        hours_in_weekday = self.hours_in_weekday[idx]
        hours_in_weekend = self.hours_in_weekend[idx]
        days_in_weekday = self.days_in_weekday[idx]
        days_in_weekend = self.days_in_weekend[idx]
        return idx, bs_record, kge, hours_in_weekday, hours_in_weekend, days_in_weekday, days_in_weekend

    def __len__(self):
        return self.kge.shape[0]


LENGTH = 24*7
SHAPE_M0 =4# 7#4
SHAPE = [ (4, 6*SHAPE_M0), (4*2, 3*SHAPE_M0), (4*6, SHAPE_M0) ]
BATCH_SIZE =2000
BATCH_FIRST = False
kge_type = 'original_kge'
if kge_type == 'node':
    KGE_SIZE = 160  # 32
    NOISE_SIZE = KGE_SIZE
    type_dir = ''
    work_dir = ''
    save_dir_head = os.path.join(work_dir, type_dir)
    transfer_generation_dir = ''
    transfer_save_dir = os.path.join(transfer_generation_dir, type_dir)
    if not os.path.exists(transfer_save_dir):
        os.mkdir(transfer_save_dir)
    idx_use = np.load(os.path.join(transfer_generation_dir, 'bs_record_nj_norm.npz'))['idx_use']
    transfer_node_file = os.path.join(transfer_generation_dir,
                                      /node_embedding_KL_log_layerscat_d10.npz')
    transfer_node_data = np.load(transfer_node_file)['node_embedding'][idx_use]
    transfer_dataset = TransferDataset(data=os.path.join(transfer_generation_dir, 'bs_record_nj_norm.npz'),
                                       node_data=transfer_node_data)

    # dataset=MyDataset('/data5/huishuodi/cross-city/urban_data/shanghai/bs_record_w_4g_use.npz')#('D:\实验室\物联网云平台\\traffic_generation\\feature\\data_train.npz')
    node_file = os.path.join(work_dir, 'node_embedding_KL_log_layerscat_d14_c4.npz')
    node_data = np.load(node_file)['node_embedding']
    dataset = MyDataset(data=os.path.join(work_dir, 'bj_2000.npz'),
                        node_data=node_data)
elif kge_type == 'original_kge':
    KGE_SIZE = 32
    NOISE_SIZE = KGE_SIZE
    type_dir = ''
    work_dir = ''
    save_dir_head = os.path.join(work_dir, type_dir)
    transfer_generation_dir = ''
    transfer_save_dir = os.path.join(transfer_generation_dir, type_dir)
    if not os.path.exists(transfer_save_dir):
        os.mkdir(transfer_save_dir)
#    idx_use = np.load(os.path.join(transfer_generation_dir, 'bj_2000.npz'))['idx_use']
    transfer_node_file = os.path.join(transfer_generation_dir,
                                      'bs4graph.npz')
    transfer_node_data = np.load(transfer_node_file)['kge']
    transfer_dataset = TransferDataset(data=os.path.join(transfer_generation_dir, 'bs_record_w.npz'),
                                       node_data=transfer_node_data)

    # dataset=MyDataset('/data5/huishuodi/cross-city/urban_data/shanghai/bs_record_w_4g_use.npz')#('D:\实验室\物联网云平台\\traffic_generation\\feature\\data_train.npz')
    node_file = os.path.join(work_dir, '')
    node_data = np.load(node_file)['kge']
    dataset = MyDataset(data=os.path.join(work_dir, ''),
                        node_data=node_data)
elif kge_type == 'poi_cate_dtb':
    KGE_SIZE = 14
    NOISE_SIZE = KGE_SIZE
    type_dir = ''
    work_dir = ''
    save_dir_head = os.path.join(work_dir, type_dir)
    transfer_generation_dir = ''
    transfer_save_dir = os.path.join(transfer_generation_dir, type_dir)
    if not os.path.exists(transfer_save_dir):
        os.mkdir(transfer_save_dir)
    idx_use = np.load(os.path.join(transfer_generation_dir, 'bs_record_nj_norm.npz'))['idx_use']
    transfer_node_file = os.path.join(transfer_generation_dir,
                                      'bs4graph.npz')
    transfer_node_data = np.load(transfer_node_file)['poi_cate_dtb'][idx_use]
    transfer_dataset = TransferDataset(data=os.path.join(transfer_generation_dir, 'bs_record_nj_norm.npz'),
                                       node_data=transfer_node_data)

    # dataset=MyDataset('/data5/huishuodi/cross-city/urban_data/shanghai/bs_record_w_4g_use.npz')#('D:\实验室\物联网云平台\\traffic_generation\\feature\\data_train.npz')
    node_file = os.path.join(work_dir, 'bs4graph_4g.npz')
    node_data = np.load(node_file)['poi_cate_dtb']
    dataset = MyDataset(data=os.path.join(work_dir, 'bj_2000.npz'),
                        node_data=node_data)

DATASET_SIZE = len(dataset)
gene_size = BATCH_SIZE
TimeList = []
D_costList = []
G_costList = []
WDList = []
dst_list = []
transfer_real_dataset_list = [transfer_dataset.data['hours_in_weekday'], transfer_dataset.data['hours_in_weekend'],
                         transfer_dataset.data['days_in_weekday'],
                         transfer_dataset.data['days_in_weekend'], transfer_dataset.data['bs_record']]
transfer_real_data = transfer_real_dataset_list[4]
real_dataset = dataset.data['bs_record']#*dataset.data['bs_record_max']
data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, drop_last=True)

dropout = 0.5
num_layers = 4
nhead = [1, 1, 1]
netG = Generator(nhead=nhead, dropout=dropout, num_layers=num_layers)
netD = Discriminator(nhead=nhead, dropout=dropout, num_layers=num_layers)

TRAIN = True
TRANSFER_GENE = True
GENE = False
GENE_NUM = 20
LOAD_TRAIN = False
LAMBDA = 10
ITERS = 300#31
CRITIC_ITERS = 5


if LOAD_TRAIN:
    pretrained_netG = torch.load(save_dir_head+'/iteration-315/netG',map_location=torch.device('cpu'))
    netG.load_state_dict(pretrained_netG)




if TRAIN:
    if use_cuda:
        netD = netD.cuda(gpu)
        netG = netG.cuda(gpu)
    print(netG)
    print(netD)
    optimizerD = optim.Adam(netD.parameters(), lr=1e-5, betas=(0.5, 0.9))
    optimizerG = optim.Adam(netG.parameters(), lr=1e-5, betas=(0.5, 0.9))

    one = torch.tensor(1, dtype=torch.float32)
    mone = one * -1.0

    if use_cuda:
        one = one.cuda(gpu)
        mone = mone.cuda(gpu)
    print(time.localtime())
    for iteration in trange(ITERS):
        start_time = time.time()
        #print(time.localtime(), ' iteration: ', iteration)
        iter_d = 0
        for idx, data in enumerate(data_loader):

            ############################
            # (1) Update D network
            ###########################
            for p in netD.parameters():  # reset requires_grad
                p.requires_grad = True  # they are set to False below in netG update
            iter_d = iter_d + 1

            #id_batch = data[0]
            data_batch = data[1]
            kge_batch = data[2]
            if not(BATCH_FIRST):
                data_batch = data_batch.permute(2,0,1)

            netD.zero_grad()

            real_data = data_batch
            #kge = kge_batch
            if use_cuda:
                real_data = real_data.cuda(gpu)
                kge_batch = kge_batch.cuda(gpu)
            D_real = netD(real_data, kge_batch)
            D_real = D_real.mean()
            # print D_real
            # TODO: Waiting for the bug fix from pytorch
            D_real.backward(mone)

            # generate noise
            noise_batch = torch.randn(BATCH_SIZE, NOISE_SIZE)#, LENGTH)
            if use_cuda:
                noise_batch = noise_batch.cuda(gpu)
            #noise_kge = torch.cat((noise_batch, kge_batch.view(kge_batch.size(0),kge_batch.size(1),1).expand(-1, -1, LENGTH)), 1)
            #noisev = autograd.Variable(noise, volatile=True)
            # train with fake
            fake_data = netG(torch.cat((noise_batch, kge_batch), 1))
            D_fake = netD(fake_data, kge_batch)
            D_fake = D_fake.mean()
            # TODO: Waiting for the bug fix from pytorch
            D_fake.backward(one)

            # train with gradient penalty
            #print(fake_data.shape, real_data.shape)
            gradient_penalty = calc_gradient_penalty(netD, real_data, fake_data, kge_batch)
            gradient_penalty.backward()

            D_cost = D_fake - D_real + gradient_penalty
            Wasserstein_D = D_real - D_fake
            nn.utils.clip_grad_norm_(netD.parameters(), max_norm=0.1)
            optimizerD.step()
            #print('#######Wasserstein_D###########',Wasserstein_D)

            if iter_d%CRITIC_ITERS == 0:
                ############################
                # (2) Update G network
                ###########################
                for p in netD.parameters():
                    p.requires_grad = False  # to avoid computation
                netG.zero_grad()

                noise_batch = torch.randn(BATCH_SIZE, NOISE_SIZE)#, LENGTH)
                if use_cuda:
                    noise_batch = noise_batch.cuda(gpu)
                #noisev = autograd.Variable(noise)
                #noise_kge = torch.cat((noise_batch, kge_batch.view(kge_batch.size(0),kge_batch.size(1),1).expand(-1, -1, LENGTH)), 1)
                fake = netG(torch.cat((noise_batch, kge_batch), 1))
                G = netD(fake, kge_batch)
                G = G.mean()
                G.backward(mone)
                G_cost = -G
                nn.utils.clip_grad_norm_(netD.parameters(), max_norm=0.1)
                optimizerG.step()
                #print(G_cost, D_cost, Wasserstein_D)
                G_costList.append(G_cost.cpu().data.numpy())
                TimeList.append(time.time() - start_time)
                D_costList.append(D_cost.cpu().data.numpy())
                ##SD_costList.append(SD_cost.cpu().data.numpy())
                WDList.append(Wasserstein_D.cpu().data.numpy())

        if iteration % 30 == 0:
            save_dir = save_dir_head + '/iteration-' + str(iteration)
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)

            torch.save(netD.state_dict(), os.path.join(save_dir, 'netD'))
            ##torch.save(netSD.state_dict(), os.path.join(save_dir, 'netSD'))
            torch.save(netG.state_dict(), os.path.join(save_dir, 'netG'))
            #torch.save(netD.module.state_dict(), os.path.join(save_dir, 'netD'))
            ##torch.save(netSD.module.state_dict(), os.path.join(save_dir, 'netSD'))
            #torch.save(netG.module.state_dict(), os.path.join(save_dir, 'netG'))
            np.savez(os.path.join(save_dir, 'cost_generated.npz'), \
                     time=np.array(TimeList), \
                     D_cost=np.array(D_costList), \
                     ##SD_cost=np.array(SD_costList),\
                     G_cost=np.array(G_costList),\
                     WD=np.array(WDList))

            #generated_data = generate_data(netG, dataset.kge[random.sample(range(DATASET_SIZE), gene_size)], gene_size)
            slice_list = np.arange(0,DATASET_SIZE,BATCH_SIZE).tolist()
            slice_list.append(DATASET_SIZE)
            #print(slice_list)
            generated_data_list = []
            for ii in np.arange(len(slice_list)-1):
                #print(slice_list[ii+1]-slice_list[ii])
                generated_data_list.append(generate_data(netG, dataset.kge[slice_list[ii]:slice_list[ii+1]],
                                                         slice_list[ii+1]-slice_list[ii]).cpu().detach().view(
                    slice_list[ii+1]-slice_list[ii], -1))
            generated_data = torch.cat(generated_data_list,dim=0)
            generated_data = generated_data.view(DATASET_SIZE, -1).numpy()
            #kge_used = kge_gene.cpu().detach().numpy()
            #generated_d = generated_d.view(gene_size, LENGTH).cpu().detach().numpy()*dataset.data['bs_record_max']
            #generated_w = generated_w.view(gene_size, LENGTH).cpu().detach().numpy()*dataset.data['bs_record_max']
            #generated_m = generated_m.view(gene_size, LENGTH).cpu().detach().numpy()*dataset.data['bs_record_max']
            #squeezed_kge = squeezed_kge.cpu().detach().numpy()
            fig, ax = plt.subplots(figsize=(24, 16))
            n_bins = 100
            line_w = 2
            use_cumulative = -1
            use_log = True
            n_real, bins, patches = ax.hist(real_dataset.flatten(), n_bins, density=True, histtype='step', cumulative=use_cumulative, label='real', log=use_log, facecolor='g', linewidth=line_w)
            n_gene, bins, patches = ax.hist(generated_data.flatten(), n_bins, density=True, histtype='step', cumulative=use_cumulative, label='gene', log=use_log, facecolor='b', linewidth=line_w)
            ax.grid(True)
            ax.legend(loc='right')
            ax.set_title('Cumulative step histograms')
            ax.set_xlabel('Value')
            ax.set_ylabel('Likelihood of occurrence')
            plt.savefig(os.path.join(save_dir, 'fig_hist.jpg'))
            plt.close()
            jsd, jsd_diff = distribution_jsd(generated_data, real_dataset, save_dir=save_dir)
            rmse = pattern_freq_ratio_rmse(generated_data, real_dataset)
            dst = [jsd, jsd_diff, rmse]
            dst_list.append(dst)
            np.savez(os.path.join(save_dir, 'generated.npz'), \
            generated_data = generated_data, \
            distance = dst, \
            distances = np.array(dst_list))#, \
            #kge_used = np.array(kge_used), \
            #squeezed_kge = squeezed_kge, \
            #generated_d = generated_d, \
            #generated_w = generated_w, \
            #generated_m = generated_m)
			#np.savez(os.path.join(save_dir, 'distance.npz'), distances = np.array(dst_list))
#        if iteration % 10 == 0:
            print(G_cost, D_cost, Wasserstein_D, dst)

if TRANSFER_GENE:
    metric_results = np.zeros((GENE_NUM, 3))
    pretrained_netG = torch.load(save_dir_head + '/iteration-300/netG', map_location=torch.device('cpu'))
    netG.load_state_dict(pretrained_netG,False)
    netG = netG.cuda(gpu)
    slice_list = np.arange(0, len(transfer_dataset), BATCH_SIZE).tolist()
    slice_list.append(len(transfer_dataset))
    # print(slice_list)
    for seed in trange(GENE_NUM):
        torch.manual_seed(seed)
        generated_data_list = []
        for ii in np.arange(len(slice_list) - 1):
            #print(transfer_dataset.kge[slice_list[ii]:slice_list[ii + 1]].shape,slice_list[ii + 1] - slice_list[ii])
            generated_data_list.append(generate_data(netG, transfer_dataset.kge[slice_list[ii]:slice_list[ii + 1]].cuda(gpu),
                                                     slice_list[ii + 1] - slice_list[ii]).cpu().detach().view(
                slice_list[ii + 1] - slice_list[ii], -1))
        generated_data = torch.cat(generated_data_list, dim=0)
        generated_data = generated_data.view(len(transfer_dataset), -1).numpy()
        jsd, jsd_diff = distribution_jsd(generated_data, transfer_real_data, save_dir=transfer_save_dir)
        rmse = pattern_freq_ratio_rmse(generated_data, transfer_real_data)
        metric_results[seed, :] = [jsd, jsd_diff, rmse]
        # print('Generating finished!')
    np.savez(os.path.join(transfer_save_dir, 'metric_results.npz'),
                 metric_results=metric_results)
    print(metric_results.mean(0))

